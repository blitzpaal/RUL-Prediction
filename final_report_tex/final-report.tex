%% ISAE-SUPAERO report template for research projects 
%% V1.0
%% 2016/04/14
%% by Damien Roque
%% See http://personnel.isae.fr/damien-roque


%% This template is based on bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************

\documentclass[conference]{IEEEtran}

\usepackage[utf8]{inputenc}
\usepackage{ifthen}
\usepackage[backend=biber, style=ieee]{biblatex}
\addbibresource{references_final_report.bib}
\usepackage{hyperref}
\usepackage{url}
\usepackage[pdftex]{graphicx}
\graphicspath{{images/}}
\usepackage{tikz,filecontents}
\usetikzlibrary{matrix,calc}
\usetikzlibrary{shapes,arrows,shadings,patterns}
\usepackage{pgfplots}
\pgfplotsset{compat=newest}
\pgfplotsset{plot coordinates/math parser=false}
\newlength\figureheight
\newlength\figurewidth

\usepackage{amsfonts}
\usepackage[cmex10]{amsmath}
\usepackage{multirow}
\usepackage[acronym,indexonlyfirst,nomain]{glossaries}

% Examples of several macros
\newcommand*{\SET}[1]{\ensuremath{\boldsymbol{#1}}}
\newcommand*{\VEC}[1]{\ensuremath{\boldsymbol{\mathrm{#1}}}}
\newcommand*{\FAM}[1]{\ensuremath{\mathrm{#1}}}
\newcommand*{\MAT}[1]{\ensuremath{\boldsymbol{\mathrm{#1}}}}
\newcommand*{\OP}[1]{\ensuremath{\mathrm{#1}}}
\newcommand*{\NORM}[1]{\ensuremath{\left\|#1\right\|}}
\newcommand*{\DPR}[2]{\ensuremath{\left \langle #1,#2 \right \rangle}}

\newtheorem{theorem}{Theorem}

\newcommand{\alert}[1]{\textcolor{red}{#1}}
\usepackage[caption=false,font=footnotesize]{subfig}

% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally ized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{REMAINING USEFUL LIFE PREDICTIONS WITH DEEP LEARNING METHODS}

% for over three affiliations, or if they all won't fit within the width
% of the page, use this alternative format:
% 
\author{\IEEEauthorblockN{Thomas Guillebot de Nerville\IEEEauthorrefmark{1},
Paul Strähle\IEEEauthorrefmark{1},
Anass Akrim\IEEEauthorrefmark{2}\IEEEauthorrefmark{3} and 
Rob Vingerhoeds\IEEEauthorrefmark{2}}

\IEEEauthorblockA{\IEEEauthorrefmark{1}Institut Supérieur de l'Aéronautique et de l'Espace (ISAE-SUPAERO), Université de Toulouse, 31400 Toulouse, FRANCE\\
Email: \{thomas.guillebot-de-nerville,paul.strahle\}@student.isae-supaero.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{2}Institut Supérieur de l'Aéronautique et de l'Espace (ISAE-SUPAERO), Université de Toulouse, 31400 Toulouse, FRANCE\\
Email: \{anass.akrim,rob.vingerhoeds\}@isae-supaero.fr}
\IEEEauthorblockA{\IEEEauthorrefmark{3}Institut Clément Ader (UMR CNRS 5312) INSA/UPS/ISAE/Mines Albi, Université de Toulouse, 31400 Toulouse, FRANCE\\
Email: anass.akrim@univ-tlse3.fr}
}


%\IEEEspecialpapernotice{(Bibliography report)}
\IEEEspecialpapernotice{(Final report)}

% import the acronyms
\input{acronyms}

% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
	
% At the end update abstract
This paper aims to compare Deep Learning models to predict the \gls{rul} of a structure. These models are the following: the \gls{cnn} family with a \gls{cnn} and a \gls{tcn} architecture, the \glspl{rnn} family with a \gls{rnn}, a \glspl{lstm} and a \glspl{gru} architecture. 

The dataset consists in simulations of the crack growth on plates, based on the Paris-Erdogan law. The training dataset is composed of 10,000 structures while the testing dataset is composed of 100 structures. Each architecture is optimized and trained on respectively 100, 500 and 1,000 structures from the taining dataset. At the end, the different models are compared on their evaluation on the testing dataset, on the basis of a metric which is the accuracy in our case.

In order to optimize the architectures, a first step is to identify the fixed parameters, which do not have a significant impact on the accuracy, and the variable ones.  The variable hyperparameters are optimized with a Random Search Strategy for 100 and 500 structures for the \gls{rnn} family, the architure used for 1,000 structures being the one from the 500 structures optimization. For the \gls{cnn} family, the variable hyperparameters are optimized for 100, 500 and 1,000 structures. The optimized models are then trained with an adaptative learning rate on the three numbers of structures. The results show that within the \glspl{rnn} family, \gls{gru} gives the best result, while within the \glspl{cnn} family, the \gls{tcn} is the best model. Then, the results show that \gls{tcn} gives a better performance than \gls{gru} with an accuracy reaching $98$\%\ for 1,000 structures.
\end{abstract}

\IEEEpeerreviewmaketitle

\section{Background}
\label{sec:background}

\noindent
\subsection{Deep Learning}

In recent years Deep Learning, a subbranch of Machine Learning, has shown impressive results, especially in the fields of speech recognition, visual object recognition and object detection \cite{LeCun2015}. One requirement in using Deep Learning is the presence of sufficient amounts of data \cite{Sikorska2011}. As more data becomes available in the engineering domain there is a recent surge of interest in using Deep Learning in engineering \cite{Voulodimos2018}.

One of the strengths of Deep Learning approaches is their ability to deal with and detect complex relationships in large datasets \cite{MONTEROJIMENEZ2020539}. This strength makes their usage also interesting in the \gls{phm} domain \cite{Wu2015}. The potential of Deep Learning in \gls{phm} might not be fully exploited yet \cite{Akrim2021}.

\noindent
\subsection{Prognostics and Health Management}

According to Zio \cite{Zio2012} \gls{phm} is a field of research and application which aims at making use of past, present and future information on the environmental, operational and usage conditions of a piece of equipment in order to detect its degradation, diagnose its faults, predict and proactively manage its failures. In the context of this project only the detection of degradation and prediction of failure are relevant.

\gls{phm} models can be divided into single and multi-model approaches. Multi-model approaches are a combination of different single-model approaches. Single-model approaches can be further divided into knowledge-based, data-driven and physics-based models. Within the data-driven models there are statistical, stochastic and Machine Learning models which is the category of Deep Learning models. \cite{MONTEROJIMENEZ2020539}

\section{Interest}
\label{sec:interest}

Deep Learning has shown some impressive results when applied to \gls{rul} prediction as shown in \cite{Xu2018, Li2018, Liu2019, Yuan2016, Wu2018, Park2020} and other publications (For an overview see Akrim et al. \cite{Akrim2021}).

Within the available Deep Learning models there are two algorithms which are promising for \gls{rul} prediction: \glspl{rnn} (Little \cite{Little1996} and Hopfield  \cite{Hopfield1982}) and \glspl{cnn} (Lecun et al. \cite{Lecun1998}) \cite{Akrim2021}.

\glspl{rnn} are the common Deep Learning approach for time-dependent relationships and have therefore also achieved great interest in the \gls{phm} domain \cite{Akrim2021}. Pioneers models were developed such as \glspl{erm} \cite{Elman1990} or Jordan Networks \cite{Jordan1997}, outperforming traditional \glspl{mlp} for sequence-prediction \cite{Akrim2021}. These algorithms were then widely explored by several researchers. Among them can be cited Yan et al. \cite{Yan2007} for their work on material degradation evaluation and life prediction in 2007, and Kramti et al. \cite{Kramti2018} for having used \glspl{erm} for high-speed shaft bearing prognostics based on vibration signals \cite{Akrim2021}.

An emerging type of Deep Learning network for time-dependent relationships are \glspl{cnn} which might be able to outperform \glspl{rnn} \cite{Bai2018}. In one of the first applications of \glspl{cnn} to \gls{phm} Li et al. suggested that \glspl{cnn} can be used to obtain \gls{rul} prognoses for machinery \cite{Li2018}. The model was applied to the C-MAPSS dataset \cite{Saxena2008} and outperformed state-of-the-art prognostics approaches including \gls{rnn} and \gls{lstm} models.

In this work a synthetic dataset is used as according to Fink et al. \cite{Fink2020} the use of simulation environments and adaption to real-life applications is a promising future research approach as the data will more likely be sufficient in the source domain. The synthetic dataset used for this study aims at using this advantage.

The synthetic dataset matches strain gauge data to the \gls{rul}. The use of strain gauges as an input to an \gls{phm} model is interesting as they play a vital role in \gls{phm} in general \cite{Tinga2019} and specifically for the aircraft domain \cite{Timothy2009}.

\section{Aim}
\label{sec:aim}

The goal of this study is to predict the \gls{rul} of precracked plates based on strain gauge measurements using Deep Learning. 
For the application of Deep Learning to crack growth for \gls{rul} prediction based on strain gauge measurements no results in the literature could be found. This study attempts to fill this research gap.

In a work done by Akrim \cite{Akrim} the Paris-Erdogan Law \cite{Paris1963} was used to create a synthetic dataset of crack growth to train the model. The dataset consists of strain data from virtual strain gauges placed in the area around the crack and the corresponding \gls{rul} of the fuselage panels.

The Paris-Erdogan Law is applied to an infinite plate with a central horizontal crack. The strain gauges are placed in a 45°-Angle at the positions shown in Figure \ref{fig:strain_gauge_positions} relative to the center of the plate where the crack is located.

\begin{figure}[htp]
	\centering
	\includegraphics[width=6cm]{python/strain_gauges_position.pdf}
	\caption{Position of the strain gauges relative to the crack}
	\label{fig:strain_gauge_positions}
\end{figure}

The crack length in the Paris-Erdogan Law is a function of: $k$ number of cylces, $\Delta \sigma$ stress range, $m, C$ material constants, $a_0$ initial crack length. By calculating the crack growth until the crack length $a$ reaches the critical crack length $a_{crit}=(\frac{K_{IC}}{\Delta \sigma \sqrt{\pi}})^2$ the simulation can determine the \gls{rul} for each simulated cycle. For the simulations the stress range $\Delta \sigma$ is kept constant while the initial crack length $a_0$ and the material parameters $m$ and $C$ are drawn from a normal distribution. $ 10000 $ structures are generated this way to form the training and validation dataset. For the testing dataset 100 structures are generated. The output of theses simulations which form the input for the deep learning models is a table matching the strains from the strain gauges to the \gls{rul} calculated by the simulations. To reduce the dataset the results are only saved to the table every 500 cycles. Table \ref{tab:sliding_window_approach} shows the structure of the generated training and validation dataset. The label $ ID $ denotes the specific structures.

\begin{table}[htp]
	\centering
	\caption{Training and validation dataset structure}
	\label{tab:sliding_window_approach}
	\begin{tabular}{lllllll}
		$ \boldsymbol{t} $ & $ \boldsymbol{ID} $ & $ \boldsymbol{cycle} $ & $ \boldsymbol{\epsilon_1} $     & $ \boldsymbol{\epsilon_2} $     & $ \boldsymbol{\epsilon_3} $     & $ \boldsymbol{RUL_{bin}} $   \\
		\hline
		$ 1 $ & $ 1 $  & $ 0 $     & $ \epsilon_{1,1} $ & $ \epsilon_{2,1} $ &  $ \epsilon_{3,1} $ &  $ RUL_{bin,1} $ \\
		$ 2 $ & $ 1 $  & $ 500 $   & $ \epsilon_{1,2} $ & $ \epsilon_{2,2} $ & $ \epsilon_{3,2} $ & $ RUL_{bin,2} $ \\
		$ 3 $ & $ 1 $  & $ 1000 $  & $ \epsilon_{1,3} $ & $ \epsilon_{2,3} $ & $ \epsilon_{3,3} $ & $ RUL_{bin,3} $ \\
		$ 4 $ & $ 1 $  & $ 1500 $  & $ \epsilon_{1,4} $ & $ \epsilon_{2,4} $ & $ \epsilon_{3,4} $ & $ RUL_{bin,4} $ \\
		\vdots & \vdots & \vdots & \vdots & \vdots &\vdots & \vdots \\
		$ n $ & $ 10000 $  & $ N_{f,10000} $  & $ \epsilon_{1,5} $ & $ \epsilon_{2,5} $ & $ \epsilon_{3,5} $ & $ RUL_{bin,5} $
	\end{tabular}
\end{table}

The aim of this work is to predict the \gls{rul} based on current and past information from the 3 strain gauges using deep learning models. Different Deep Learning model architectures are to be used for that. The developed models must be trained and optimized before a comparison between them is made.

\section{Methods}
\label{sec:methods}

\noindent
\subsection{Available Dataset}

% Explanation of classification and preparation of dataset (Thomas)
The provided datasets were separated in two distinguished parts, namely training and testing dataset. The first consists in 10,000 structures with different initial crack. All the models were trained on this dataset using only a part of it, respectively 100, 500 and 1,000 structures. While training, the last 100 structures of the training dataset were used as a validation dataset.

Then, the 100 structures from the second dataset were used to evaluate the performances of our models.


As it showed promising results in other applications of Deep Learning to \gls{phm} (\cite{Liu2019a, Xiao2016}), classification is an interesting alternative to regression for \gls{rul} prediction. Instead of trying to predict an exact \gls{rul} value the goal is to predict the correct \gls{rul} class with a lower and upper bound for the \gls{rul}. The generated dataset was composed of 80,000 cycles at most. Therefore the chosen strategy has been to create 20 intervals following a parabolic equation. The last class would be for a \gls{rul} over 80,000 cycles away.


\begin{figure}[htp]
	\centering
	\includegraphics[width=6cm]{RUL_Classification.png}
	\caption{\gls{rul} classification strategy}
	\label{fig:RNN-classification}
\end{figure}


To prepare the data for the \gls{cnn} a sliding window approach was used. This approach maps the \gls{rul} at time $ t $ onto the current and past time steps of the input features $ [x_{t-N+1}, x_{t-N+2},..., x_{t-1}, x_t] $ where $ N $ is the length of the sliding window. The resulting input matrix therefore has the dimension $ (n-N) \times N \times k $ where $ n $ is the number of samples and $ k $ is the number of features. Figure \ref{fig:input_matrix} shows an example of how the time series data of the strain gauges from Table \ref{tab:sliding_window_approach} is mapped to the input matrix.

\begin{figure}[htp]
	\centering
	\input{images/input_matrix.tex}
	\caption{Input matrix for the neural networks}
	\label{fig:input_matrix}
\end{figure}

The output matrix shown in Figure \ref{fig:output_matrix} is only one dimensional. It is only composed of the \gls{rul} bins which are mapped to the corresponding rows in the input matrix the dimension therefore is $ (n-N) $.

\begin{figure}[htp]
	\centering
	\input{images/output_matrix.tex}
	\caption{Input matrix for the neural networks}
	\label{fig:output_matrix}
\end{figure}

As an input for the different models, datasets had to be normalized. While a specific layer was added at the beggining of \gls{cnn} and \gls{tcn} architectures to do so, the preprocessed data had to be normalized before being fed to the \glspl{rnn} models.

% Add details on the explanation of the RNN Deep Learning models (Thomas)

\noindent
\subsection{Recurrent Neural Networks}

The common type of Deep Learning model for time series prediction are \glspl{rnn} \cite{Bai2018}. Due to their ability to store information within the cell, it can better remember information of a time-dependant data. 


\begin{figure}[htp]
	\centering
	\includegraphics[width=6cm]{RNN-cell-architecture.png}
	\caption{RNN cell architecture. a. A simple RNN unit; b. an LSTM unit \cite{Chen2021}}
	\label{fig:RNN-classification}
\end{figure}

However, standard \glspl{rnn} have some major drawbacks, such as the vanishing or exploding gradient problem, which limit their application \cite{Bengio1994}. \gls{lstm} networks (Hochreiter and Schmidhuber \cite{Hochreiter1997}) avoid this problem and have established themselves as one of the most used Deep Learning model types, especially for \gls{nlp} \cite{Wu2016}. For these reasons \glspl{lstm} will be one of the investigated \gls{rnn} approaches in this project. Another investigated \gls{rnn} approach is the \glspl{gru}. It is a simplified version of the \gls{lstm}. Due to this simplicity it has been gaining in popularity in recent years \cite{Rana2016}. 

The key idea to \gls{gru} and \gls{lstm} is the memory cell which allows the network to remember information without much loss.

\gls{lstm} architecture consists in an input gate, a forget gate and an output gate. The input gate decides to add new information from the present input to the present cell state scaled by how much it wishes to add them. The forget gate allows the cell to know how to partially forget the previous cell state. Then, the output gate decides what to output from the new celle state.

For the \gls{gru} architecture, only two gates are implemented. The update gate decides the portion of updated candidate needed to calculate the current cell state, which in turn also decides the portion of the previous cell state retained. The relevance gate calculates how relevant the previous information is, and then is used in the candidate for the updated value.

Then, a simple \gls{rnn} architecture will be explored to evaluate the possible benefits of more complex structures. An \gls{rnn} unit only have a tanh layer after combining the cell state and the candidate.






% Add details on the explenation of the CNN Deep Learning models (Paul)

\noindent
\subsection{Convolutional Neural Networks}

Recent results suggest that \glspl{cnn} can match or even outperform \glspl{rnn} in time series related tasks \cite{Bai2018}. The second major focus of this work are therefore \gls{cnn} models.

The common \gls{cnn} models deal with 2-Dimensional data as input such as pictures. The time sequence data used for \gls{phm} is in 1-Dimensional format. For this application 1D-\gls{cnn} have been introduced. The key differences between them and the 2D-\glspl{cnn} are that their input data is reduced by one dimension and the convolution filter only slides in one dimension \cite{Akrim2021}. Figure \ref{fig:1D_cnn_architecture} shows an example of a simple \gls{cnn} architecture with an illustration of the filter sliding over the time series. 

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{1D_CNN_Architecture.png}
	\caption{Example of an 1D-\gls{cnn} architecture \cite{Sayyad}}
	\label{fig:1D_cnn_architecture}
\end{figure}

Besides the general \gls{cnn} architectures \glspl{tcn} (Bai et al. \cite{Bai2018}) are investigated in this work. A \gls{tcn} is a specific \gls{cnn} architecture that tries to replicate some of the best practices for \gls{cnn} architectures. As depicted in Fig. \ref{fig:tcn_architecture} a \gls{tcn} is composed of multiple layers which include dilation. The dilation for each layer can be set arbitrary but it is common practice to use multiples of $ 2 $ for it. Through dilation \glspl{tcn} can increase their receptive field and therefore capture relationships over longer time sequences. One \gls{tcn} layer is composed of a \gls{tcn} residual block (see Figure \ref{fig:tcn_block} (a)). One block consists of two dilated convolutional layers. The activation function for the layers is always the ReLu function. The residual blocks include dropout and normalization layers between the convolutional layers. The blocks also include an optional skip connection with a 1x1 convolutional layer. Figure \ref{fig:tcn_block} (b) shows an exemplary \gls{tcn} block with a kernel size of $ k = 3 $ and a dilation of $ d = 1 $. For more details on \glspl{tcn} see \cite{Bai2018}.

\begin{figure}[htp]
	\centering
	\includegraphics[width=7cm]{tcn_architecture.pdf}
	\caption{\gls{tcn} with dilation factors d = 1; 2; 4 and filter size k = 3 \cite{Bai2018}}
	\label{fig:tcn_architecture}
\end{figure}

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{tcn_block.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{tcn_block_example.pdf}}
	\caption{\gls{tcn} block elements, (a): Generic \gls{tcn} block, (b): Example for a \gls{tcn} block \cite{Bai2018}}
	\label{fig:tcn_block}
\end{figure}

% Maybe list the used frameworks (Keras, Tensoflow etc.) (Paul)

% Maybe write about the execution environment (Used GPUs etc.) (Paul)

\noindent
\subsection{Metric and loss function}

To evaluate the performance of developed models a metric is needed. As the networks are used to perform classification instead of regression the accuracy metric is used. This metric calculates how many of the predictions made with the dataset are correct (e.g. the right \gls{rul} range is predicted)

\begin{equation}
	\label{eq:categorical-cross-entrophy}
	Accuracy = \frac{Number \: of \: correct \: predictions}{Total \: number \: of \: predictions}.
\end{equation}

This metric is applied during training and validation to compare different models against each other. The final assessment of the models is made by applying the accuracy metric to the testing dataset.

The used loss function which is used as the objective function for minimization by the backpropagation algorithm is the categorical crossentrophy loss function

\begin{equation}
	\label{eq:categorical-cross-entrophy}
	CE = -log(\frac{e^{s_p}}{\sum_{j}^{C} e^{s_j}})	
\end{equation}

where $ s_j $ is the output of the network for Class $ j $ with $ s_p $ beeing the positive class.

% Details on the hyperparameter optimization strategies (how the models are trained and how their parameters are optimised) (Paul)

\noindent
\subsection{Hyperparameter Optimization}

To optimize the proposed network architectures a hyperparameter optimization is performed. The hyperparameters include the parameters for the training process (Learning rate and batch size), network parameters (Dropout rate, activation funciton etc.) and the network architecture itself (number of layers, size of the layers etc.).

There are different strategies for hyperparameter optimization which can be divided in Model-Free and Model-Based approaches. For this study only Model-Free approaches are considered as they are more common and easier to apply. The two common approaches are grid search and random search \cite{Feurer2019}. 

For grid search the user defines a discrete number of values for each parameter. The algorithm then tries out all possible combinations of these sets of values. This quickly leads to an exesive amount of training runs that need to be done as the number of possible combinations $ B $ grows exponentially with the dimensionality $ d $ of the search space. If the number of values per parameter is $ n = 3 $ and the dimensionality is just $ d = 5 $ the number of combinations already equals $ B = n^d = 243 $.

Random search works with a fixed number of evaluations where for each evaluation a value randomly is selected from a predefined set of values for each parameter. The set of values can either be continuous with a lower and upper bound for the values or a discrete set of predefined values. Random search works better than grid search when some parameters are more important than others, which is a property that holds true in many cases \cite{Feurer2019}. Figure \ref{fig:grid_search_random_search} illustrates this property for one important and one unimportant parameter. With a fixed amount of $ B $ evaluations random search can evaluate up to $ B $ different values for each parameter. Whereas grid search is limited to $ B^{1/N} $ values per parameter. It is expected that the networks in this study behave like most networks with regard to the varying importance of the hyperparameters and therefore random search is choosen for hyperparameter optimization.

\begin{figure}[htp]
	\centering
	\includegraphics[width=7cm]{grid_search_random_search.pdf}
	\caption{Comparison of grid search and random search for an important and a unimportant parameter with 9 evaluations \cite{Feurer2019}}
	\label{fig:grid_search_random_search}
\end{figure}

To select the best model after performing the hyperparameter optimization the accuracy of the model on the validation dataset is used.

\noindent
\subsection{Fine Tuning}

After the optimum parameters for a model are found the model can be further improved by running more training epochs. To do so the models are trained with \gls{lrdecay}. The best model from the hyperparameter optimization is taken and trained for a predefined number of epochs on the learning rate which was identified as the best one on the optimization. The learning rate is then lowered before the model is trained again for a fixed number of epochs. This step is repeated until convergence is achieved and no more significant improvements are visible. By using this approach the model in the early stages of training is less likely to get stuck in a local minimum and explores a wider range of possible configurations. As the training comes closer to an optimum the decaying learning rate helps with convergence and avoid oscillations. Besides this common beliefs there are probably more reasons to the effectiveness of \gls{lrdecay}. According to You et al. \cite{You2019} is the initially large learning rate preventing the network from memorizing noisy data while the decaying learnig rate helps with learning complex patterns in the dataset.


\section{Results}
\label{sec:results}

% Show the used model architectures for rnn and cnn and why they are used (Paul and Thomas)

\noindent
\subsection{Model architectures}

Based on the \gls{cnn} architecture of Li et al. \cite{Li2018} which was used for aero-engine \gls{rul} prediction a vanilla \gls{cnn} architecture is generated. The first layer of the architecture is for layer normalization. Afterwards a flexible number of 1D-convolutional layers is added which all have the same number of filters and an identical kernel size. The output of the convolutional layers is flattend before the final dense output layer which has $ 20 $ nodes to match the number of \gls{rul} classes $ C = 20 $. As an example Figure \ref{fig:cnn_architecture_1000_structures} shows the final architecture after the hyperparameter optimization with 1000 training structures.

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.5\textwidth]{cnn_architecture_1000_structures.pdf}
	\caption{\gls{cnn} architecture after hyperparameter optimization with 1000 training structures}
	\label{fig:cnn_architecture_1000_structures}
\end{figure}

A \gls{tcn} architecture template is generated based on the results of Liu et al. \cite{Liu2019} who used the network for \gls{rul} prediction of roller bearings. The architecture consists of multiple \gls{tcn} residual blocks with a dilation increasing with multiples of 2 with the first layer having a dilation of $ d = 1 $. Each block features the same number of filters with an identical kernel size. The next layer after the \gls{tcn} blocks is the dense output layer which has $ 20 $ nodes to match the number of \gls{rul} classes $ C = 20 $. The skip connection layer is activated for all residual blocks.

All \glspl{rnn} architectures were based on the same template. The dataset being noramlized before being fed to the models, there were no need for a specific layer of normalization. A flexible number of layers were implemented in the networks, as well as the number of cells in each layer, scaled by a power of 2. A possible dropout and recurrent dropout could also be added for each layer. At the end of the architecture, a dense layer with 20 units were added in order to match the number of \gls{rul} classes $ C = 20 $.  

% Give an overview of the results of the hyperparameter optimisation strategies for the rnn and cnn networks (Paul and Thomas)

\noindent
\subsection{Hyperparameters optimization}

The hyperparameter optimization is performed using random search as described in \ref{sec:methods}. The used training dataset consists of $ 100 $, $ 500 $ or $ 1000 $ structures. For validation always $ 100 $ structures different to the training structures are used. The model selection is based on the accuracy of the models on the validation dataset. Not all available parameters of the networks are considered for the optimization as that would increase the computational effort drastically. The reduction also helps to shift the focus of the optimization on the important parameters and avoid varying parameters which have no or only little influence on the accuracy of the network.

\subsubsection{\gls{cnn} and \gls{tcn} hyperparameters optimization}
Table \ref{tab:fixed_parameters_cnn_optimization} lists the fixed parameters of the vanilla \gls{cnn} optimization. The sliding window length includes multiple values as the influence of that parameter is studied by performing multiple optimization runs with different lengths of the sliding window.

\begin{table}[htp]
	\centering
	\caption{Fixed parameters for vanilla \gls{cnn} hyperparameter optimization}
	\label{tab:fixed_parameters_cnn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value} \\
		\hline
		Batch size & $ 4096 $ \\
		Sliding window length & $ \{5, 10, 15, 20, 30, 40\} $ \\
		Activation function & ReLU \\
		Padding & Padding with zeros \\
		Dropout & No dropout
	\end{tabular}
\end{table}

Table \ref{tab:variable_parameters_cnn_optimization} lists the variable parameters and their set of possible values for the \gls{cnn} hyperparameter optimization. The chosen range for the parameters is the result of a preliminary analysis to identify meaningful boundaries for them.

\begin{table}[htp]
	\centering
	\caption{Variable parameters for vanilla \gls{cnn} hyperparameter optimization}
	\label{tab:variable_parameters_cnn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value set} \\
		\hline
		Number of convolutional layers & $ \{2, 4, 6, 8\} $ \\
		Number of filters per layer & $ \{15, 20, 25, 30, 35, 40, 45, 50\} $ \\
		Kernel size & $ \{3, 6, 9, 12\} $ \\
		Learning rate & $ \{10^{-2}, 10^{-3}\} $
	\end{tabular}
\end{table}

For the vanilla \gls{cnn} the hyperparameter optimization is performed on $ 100 $, $ 500 $ and $ 1000 $  training structures. For the smallest dataset of $ 100 $ structures the optimization is performed on the full range of different sliding window lengths as listed in Table \ref{tab:variable_parameters_cnn_optimization}. The optimizations on $ 500 $ and $ 1000 $ structures are only performed with a sliding window size of $ N = 30 $. All optimization runs are performed for 500 training epochs and with a total of $ B = 100 $ evaluations.

Figure \ref{fig:influence_sequence_length_cnn} shows the achieved maximum validation accuracy after the hyperparameter optimization for different lengths of the sliding window. The accuracy increases approximately linear with size of the sliding window although the absolute value of the improvement remains relatively small. The size of the sliding window can not be increased above a size of $ 40 $ as then the sliding window would be larger than the timeseries of some of the structures. A too big sliding window besides that has the disadvantage that it can only be applied if data is already collected for at least as many timesteps as the sliding window requires. This means that there could be situations in which the first predictions can only be made just before the part fails or in the worst case after it fails. As a result of these considerations and the neglectible difference betwwen the sliding window sizes of $ 30 $ and $ 40 $ a size of $ N = 30 $ is choosen for all further hyperparameter optimizations runs wiht the vanilla \gls{cnn} on $ 500 $ and $ 1000 $ structures.

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.35\textwidth]{python/influence_sequence_length_cnn.pdf}
	\caption{Vanilla \gls{cnn} hyperparameter optimization for different sliding window sizes for training on $ 100 $ structures}
	\label{fig:influence_sequence_length_cnn}
\end{figure}

Table \ref{tab:hyperparameters_100_structures} shows the different architectures form the hyperparameter optimization that achieved the best results with the respective sliding window length on $ 100 $ training structures. There is no clear trend for the network architecture in relation to the sliding window size. This is backed by the fact that the 2nd and 3rd best networks for each respective hyperparameter optimization can have significantly different architectures to the best one. This observations lead to the conclusion that there are many different network architectures that can lead equal results.

\begin{table}[htp]
	\centering
	\caption{Best models of the vanilla \gls{cnn} hyperparameter optimization for different sliding window lengths with training on $ 100 $ structures}
	\label{tab:hyperparameters_100_structures}
	\setlength{\tabcolsep}{3pt} % Default value: 6pt
	\begin{tabular}{p{2.5cm}|llllll}
		Sliding window size & $ 5 $ & $ 10 $ & $ 15 $ & $ 20 $ & $ 30 $ & $ 40 $ \\
		\hline
		Number of convolutional layers & $ 4 $ & $ 2 $ & $ 4 $ & $ 2 $ & $ 6 $ & $ 6 $ \\
		Number of filters per layer & $ 25 $ & $ 35 $ & $ 30 $ & $ 45 $ & $ 20 $ & $ 20 $ \\
		Kernel size & $ 3 $ & $ 6 $ & $ 9 $ & $ 9 $ & $ 6 $ & $ 3 $ \\
		Learning rate & $ 10^{-2} $ & $ 10^{-2} $ & $ 10^{-2} $ & $ 10^{-2} $ & $ 10^{-2} $ & $ 10^{-2} $ \\
		\hline
		Validation accuracy & $ 0.698 $ & $ 0.706 $ & $ 0.709 $ & $ 0.714 $ & $ 0.724 $ & $ 0.734 $
	\end{tabular}
\end{table}

Figure \ref{fig:accuracy_100_structures_random_search_cnn} shows the history of the training and validation accuracy for some of the different sliding window sizes for the training with $ 100 $ structures. There is little to no overfitting on the training dataset for all of the shown training runs. The $ 500 $ training epochs lead to a sufficient convergence for all the runs.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_10_CNN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_20_CNN.pdf}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_30_CNN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_40_CNN.pdf}}
	\caption{Training and validation accuracy history for the best model of the vanilla \gls{cnn} hyperparameter optimization on $ 100 $ training structures with a sliding window size of: (a): $ 10 $, (b): $ 20 $, (c): $ 30 $, (d): $ 40 $}
	\label{fig:accuracy_100_structures_random_search_cnn}
\end{figure}

The hyperparameter optimizations of the vanilla \gls{cnn} with $ 500 $ training structures and a sliding window size of $ 30 $ leads to an improvement of the validation accuracy. Showing that the network architecture if provided with more data can better learn the relationships within the dataset. An increase of the training dataset to $ 1000 $ structures leads to to further improvement. This is a result of the lower learning rate of this network which has not fully converged yet (see Figure \ref{fig:accuracy_500_1000_structures_random_search_CNN})  Table \ref{tab:hyperparameters_100_500_1000_structures_CNN} depicts the best network architecture with the achieved validation accuracy for $ 100 $, $ 500 $ and $ 1000 $ training structures with a sliding window size of $ 30 $. There is not clear trend visible in the network architectures indicating that there are many different possible architectures which achieve the same results. This hypothesis is supported by some of the other architectures created by the hyperparameter optimization which achieve similar results with distinctly different architectures.

\begin{table}[htp]
	\centering
	\caption{Best models of the vanilla \gls{cnn} hyperparameter optimization for $ 100 $, $ 500 $ and $ 1000 $ training structures}
	\label{tab:hyperparameters_100_500_1000_structures_CNN}
	\begin{tabular}{p{2.5cm}|llllll}
		Training structures & $ 100 $ & $ 500 $ & $ 1000 $ \\
		\hline
		Sliding window size & \multicolumn{3}{c}{$ 30 $} \\
		\hline
		Number of convolutional layers & $ 6 $ & $ 2 $ & $ 4 $ \\
		Number of filters per layer & $ 20 $ & $ 40 $ & $ 25 $ \\
		Kernel size & $ 6 $ & $ 12 $ & $ 9 $ \\
		Learning rate & $ 10^{-2} $ & $ 10^{-2} $ & $ 10^{-3} $ \\
		\hline
		Validation accuracy & $ 0.724 $ & $ 0.788 $ & $ 0.789 $
	\end{tabular}
\end{table}

Figure \ref{fig:accuracy_500_1000_structures_random_search_CNN} shows the training history for the best models of the vanilla \gls{cnn} hyperparameter optimization when training on $ 500 $ and $ 1000 $ structures respectively. None of the two models shows significant overfitting on the training dataset. Both models are not yet fully converged and therfore have room for improvement. This potential is exploited in the model fine tuning in section XXX. The training history of the model trained on $ 500 $ epochs shows an instability at around $ 480 $ epochs. Indicating that the learning rate should be reduced which is done in the subsequent model fine tuning.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_500_structures_random_search_CNN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_1000_structures_random_search_CNN.pdf}}
	\caption{Training and validation accuracy history for the best model of the vanilla \gls{cnn} hyperparameter optimization on (a): $ 500 $ and (b): $ 1000 $ training structures}
	\label{fig:accuracy_500_1000_structures_random_search_CNN}
\end{figure}


Table \ref{tab:fixed_parameters_tcn_optimization} lists the fixed parameters of the \gls{tcn} optimization. The sliding window length includes multiple values as the influence of that parameter is studied by performing multiple optimization runs with different lengths of the sliding window.

\begin{table}[htp]
	\centering
	\caption{Fixed parameters for \gls{tcn} hyperparameter optimization}
	\label{tab:fixed_parameters_tcn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value} \\
		\hline
		Batch size & $ 4096 $ \\
		Sliding window length & $ \{10, 15, 20, 30\} $ \\
		Activation function & ReLU \\
		Padding & Causal padding \\
		Normalization & Weight normalization
	\end{tabular}
\end{table}

Table \ref{tab:variable_parameters_tcn_optimization} lists the variable parameters and their set of possible values for the \gls{tcn} hyperparameter optimization. The chosen range for the parameters is the result of a preliminary analysis to identify meaningful boundaries for them. The indicated number of layers directly corresponds the the maximum dilation as the dilations rises with multiples of two for each \gls{tcn} block. For e.g. 4 blocks the dilation in the blocks is $ [1, 2, 4, 8] $.


\begin{table}[htp]
	\centering
	\caption{Variable parameters for \gls{tcn} hyperparameter optimization}
	\label{tab:variable_parameters_tcn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value set} \\
		\hline
		Number of layers & $ \{4, 6, 8, 10\} $ \\
		Number of filters per layer & $ \{15, 20, 25, 30, 35, 40, 45, 50\} $ \\
		Kernel size & $ \{3, 6, 9, 12\} $ \\
		Dropout rate & $ \{0.0, 0.1, 0.2\} $ \\
		Learning rate & $ \{10^{-2}, 10^{-3}\} $
	\end{tabular}
\end{table}

As for the vanilla \gls{cnn} the hyperparameter optimization for the \gls{tcn} is performed on $ 100 $, $ 500 $ and $ 1000 $  training structures. For the smallest dataset of $ 100 $ structures the optimization is performed on the full range of different sliding window lengths as listed in Table \ref{tab:variable_parameters_cnn_optimization}. The optimizations on $ 500 $ and $ 1000 $ structures are only performed with a sliding window size of $ N = 30 $. All optimization runs are performed for 500 training epochs and with a total of $ B = 50 $ evaluations. The evaluations are reduced compared to the \gls{cnn} optimization because of the increased computational effort for the \gls{tcn}.

Figure \ref{fig:influence_sequence_length_tcn} shows the achieved maximum validation accuracy after the hyperparameter optimization for different lengths of the sliding window. There is a significant increase of the accuracy between a length of $ 15 $ and $ 30 $. The other differences seem to be the result of scatter as a result of the training as well as the random search which does lead to different network architectures in each optimization run. To keep a big enough distance to the drop in the accuarcy for windows sizes $ \leq 20 $ a sliding window size of $ N = 30 $ is chosen for all further hyperparameter optimizations with the \gls{tcn} on $ 500 $ and $ 1000 $ structures.

\begin{figure}[htp]
	\centering
	\includegraphics[width=0.35\textwidth]{python/influence_sequence_length_tcn.pdf}
	\caption{\gls{tcn} hyperparameter optimization for different sliding window sizes for training on $ 100 $ structures}
	\label{fig:influence_sequence_length_tcn}
\end{figure}

Figure \ref{fig:accuracy_100_structures_random_search_cnn} shows the history of the training and validation accuracy for different sliding window sizes for the training with $ 100 $ structures. There is little to no overfitting on the training dataset for all of the training runs. The $ 500 $ training epochs lead to a sufficient convergence for all the runs.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_10_TCN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_15_TCN.pdf}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_20_TCN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_random_search_30_TCN.pdf}}
	\caption{Training and validation accuracy history for the best model of the \gls{tcn} hyperparameter optimization on $ 100 $ training structures with a sliding window size of: (a): $ 10 $, (b): $ 15 $, (c): $ 20 $, (d): $ 30 $}
	\label{fig:accuracy_100_structures_random_search_tcn}
\end{figure}

Performing the hyperparameter optimization of the \gls{tcn} with $ 500 $ and $ 1000 $ structures respectively leads to significant improvements of the validation accuracy. This indicates that the network is able to better learn the relations in the dataset when feed with more data. Table \ref{tab:hyperparameters_100_500_1000_structures_TCN} depicts the best network architecture with the achieved validation accuracy for $ 100 $, $ 500 $ and $ 1000 $ training structures with a sliding window size of $ 30 $. The network architectures are very similar which shows that independent of the used training dataset a similar architecture works best.

\begin{table}[htp]
	\centering
	\caption{Best models of the \gls{tcn} hyperparameter optimization for $ 100 $, $ 500 $ and $ 1000 $ training structures}
	\label{tab:hyperparameters_100_500_1000_structures_TCN}
	\begin{tabular}{p{2.5cm}|llllll}
		Training structures & $ 100 $ & $ 500 $ & $ 1000 $ \\
		\hline
		Sliding window size & \multicolumn{3}{c}{$ 30 $} \\
		\hline
		Number of convolutional layers & $ 8 $ & $ 8 $ & $ 8 $ \\
		Number of filters per layer & $ 30 $ & $ 25 $ & $ 35 $ \\
		Kernel size & $ 2 $ & $ 2 $ & $ 2 $ \\
		Dropout rate & $ 0.0 $ & $ 0.0 $ & $ 0.0 $ \\
		Learning rate & $ 10^{-3} $ & $ 10^{-2} $ & $ 10^{-2} $ \\
		\hline
		Validation accuracy & $ 0.772 $ & $ 0.883 $ & $ 0.908 $
	\end{tabular}
\end{table}

Figure \ref{fig:accuracy_500_1000_structures_random_search_CNN} shows the training history for the best models of the vanilla \gls{cnn} hyperparameter optimization when training on $ 500 $ and $ 1000 $ structures respectively. None of the two models shows significant overfitting on the training dataset. Both models are not yet fully converged and therfore have room for improvement. This potential is exploited in the model fine tuning in section XXX. The training history of the model trained on $ 500 $ epochs shows an instability at around $ 480 $ epochs. Indicating that the learning rate should be reduced which is done in the subsequent model fine tuning.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_500_structures_random_search_TCN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_1000_structures_random_search_TCN.pdf}}
	\caption{Training and validation accuracy history for the best model of the \gls{tcn} hyperparameter optimization on (a): $ 500 $ and (b): $ 1000 $ training structures}
	\label{fig:accuracy_500_1000_structures_random_search_TCN}
\end{figure}




\subsubsection{\gls{rnn}, \gls{lstm} and \gls{gru} hyperparameters optimization}
Following the samed method used for the \gls{cnn} and \gls{tcn} hyperparameters optimization, fixed parameters were previously set for each architecture, to avoid searching for hyperparameters which would not be relevant in the accuracy improvement. Those fixed parameters are the same for the \gls{rnn}, \gls{lstm} and \gls{gru} architectures, and can be found in Table \ref{tab:fixed_parameters_rnn_optimization}.


\begin{table}[htp]
	\centering
	\caption{Fixed parameters for \gls{rnn} hyperparameters optimization}
	\label{tab:fixed_parameters_rnn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value} \\
		\hline
		Batch size & $ 4096 $ \\
		Activation function & ReLU \\
		Epochs & $ 500 $ \\
		Sequence length & $ 30 $ 
	\end{tabular}
\end{table}


Once those parameters fixed, usefull hyperparameters were identified. These are the ones which can have a significant impact on the accuracy improvement. They were identified on $ 100 $ structures, and then reused for $ 500 $ structures. Due to a lack of time, only the best architecture identified from the optimization on $ 500 $ structures was used to train on $ 1000 $ structures, and then evaluate the performance. It appeared that the variable hyperparameters to try were the same for \gls{rnn}, \gls{lstm} and \gls{gru}. They can be found in Table \ref{tab:variable_parameters_rnn_optimization}.

\begin{table}[htp]
	\centering
	\caption{Variable parameters for vanilla \gls{cnn} hyperparameter optimization}
	\label{tab:variable_parameters_rnn_optimization}
	\begin{tabular}{ll}
		\textbf{Parameter} & \textbf{Value set} \\
		\hline
		Number of hidden layers & $ \{1, 2, 3\} $ \\
		Number of cells per layer & $ \{32, 64, 128, 256\} $ \\
		Dropout rate & $ \{0, 0.1\} $ \\
		Recurrent Dropout rate & $ \{0, 0.1\} $ \\
		Learning rate & $ \{10^{-2}, 10^{-3}\} $
	\end{tabular}
\end{table}

The Random Search strategy were then used to optimize those hyperparameters on $ 100 $ structures and $ 500 $ structures, with a fixed sliding window size of $30$. The Learning Rate is not reported for the Fine Tuning will use an adaptative Learning Rate strategy to optimize the performance of the final training. The results of hyperparameters optimization can be found in Table \ref{tab:results_parameters_rnn_optimization} for both \gls{rnn}, \gls{lstm} and \gls{gru}.



\begin{table}[htp]
	\centering
	\caption{Results of the hyperparameters optimization for $ 100 $ and $ 500 $ structures}
	\label{tab:results_parameters_rnn_optimization}
	\begin{tabular}{p{2.5cm}|lll|lll}
		Training structures & \multicolumn{3}{c}{$ 100 $} & \multicolumn{3}{c}{$ 500 $} \\
		\hline
		Type of architecture & \gls{rnn} & \gls{lstm} & \gls{gru} & \gls{rnn} & \gls{lstm} & \gls{gru}\\
		\hline
		Number of hidden layers & $3$ & $2$ & $3$ & $3$ & $2$ & $2$ \\
		Number of cells per layer & $32$ & $64$ & $256$ & $32$ & $128$ & $256$ \\
		Dropout rate & $0$ & $0$ & $0$ & $0$ & $0$ & $0$ \\
		Recurrent Dropout rate & $0$ & $0$ & $0$& $0$ & $0$ & $0$ \\
		\hline
		Validation accuracy & $0.752$ & $0.688$ & $0.726$ & $0.871$ & $0.700$ & $0.814$
	\end{tabular}
\end{table}


It then can be then understood that \glspl{rnn} do not need many layers to get a good accuracy compared to \gls{cnn}. The best accuracy can be found with no more than 2 or 3 layers in this case. Indeed, too many layers and cells for this amount of data might lead the model to loose itself, along with the fact that the computational time increases drastically. The Dropout and Recurrent Dropout rates were optimized at 0\%\ meaning that the models did not overfit for this range of structures. This result is better visible as the validation accuracy follows the training accuracy on the Fine Tuning in Figure \ref{fig:accuracy_100_structures_fine_tuning_rnn}, \ref{fig:accuracy_500_structures_fine_tuning_rnn} and \ref{fig:accuracy_1000_structures_fine_tuning_rnn}.



\subsection{Fine Tuning}

As the hyperparameters were optimized, the resulting models had to be trained with an adaptative Learning rate to optimize their performance. \gls{lrdecay} started with a Learning rate of $10^{-2}$, decreasing to $10^{-3}$, $10^{-4}$ and then $10^{-5}$. With each Learning rate, models were trained on a maximum of $500$ epochs which leads to a training with a maximum of 2,000 epochs. 


For the \gls{rnn}, \gls{lstm} and \gls{gru} the results for the history of the training can be found in Figures \ref{fig:accuracy_100_structures_fine_tuning_rnn}, \ref{fig:accuracy_500_structures_fine_tuning_rnn} and \ref{fig:accuracy_1000_structures_fine_tuning_rnn}. It can be clearly observed that the adaptative learning rate strategy allows a better training for each model. Once the models trained, they were evaluated on the testing dataset. All the results for the validation accuracy and the testing accuracy can be found in Tables \ref{tab:results_fine_tuning_100_rnn}, \ref{tab:results_fine_tuning_500_rnn} and \ref{tab:results_fine_tuning_1000_rnn}. It can be noticed that the difference between the validation accuracy and the testing accuracy is notable. This could be the classification scaling which gives a larger bandwith on the last classes, while the testing dataset focuses more on the first calsses, which are smaller. This leads to an increasing possibilty of wrong prediction of the \gls{rul} on the testing dataset. One lead to explore would be then to adapt the metric to take into account this evolution of the classes bandwidth.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/RNN_100_tuned_acc.png}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/LSTM_100_tuned_acc.png}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/GRU_100_tuned_acc.png}}
	\caption{Training and validation accuarcy for $ 100 $ training structures with \gls{lrdecay}: (a): \gls{rnn}, (b): \gls{lstm}, (c): \gls{gru}}
	\label{fig:accuracy_100_structures_fine_tuning_rnn}
\end{figure}


\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/RNN_500_tuned_acc.png}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/LSTM_500_tuned_acc.png}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/GRU_500_tuned_acc.png}}
	\caption{Training and validation accuarcy for $ 500 $ training structures with \gls{lrdecay}: (a): \gls{rnn}, (b): \gls{lstm}, (c): \gls{gru}}
	\label{fig:accuracy_500_structures_fine_tuning_rnn}
\end{figure}

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/RNN_1000_tuned_acc.png}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/LSTM_1000_tuned_acc.png}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{fine_tuning/GRU_1000_tuned_acc.png}}
	\caption{Training and validation accuarcy for $ 1000 $ training structures with \gls{lrdecay}: (a): \gls{rnn}, (b): \gls{lstm}, (c): \gls{gru}}
	\label{fig:accuracy_1000_structures_fine_tuning_rnn}
\end{figure}




\begin{table}[htp]
	\centering
	\caption{Results of the fine tunning for $ 100 $ structures}
	\label{tab:results_fine_tuning_100_rnn}
	\begin{tabular}{p{2.5cm}|lll}
		Training structures & \multicolumn{3}{c}{$ 100 $} \\
		\hline
		Type of architecture & \gls{rnn} & \gls{lstm} & \gls{gru}\\
		\hline
		Validation accuracy & $0.899$ & $0.850$ & $0.783$\\
		\hline
		Testing accuracy & $0.84$ & $0.73$ & $0.70$
	\end{tabular}
\end{table}


\begin{table}[htp]
	\centering
	\caption{Results of the fine tunning for $ 500 $ structures}
	\label{tab:results_fine_tuning_500_rnn}
	\begin{tabular}{p{2.5cm}|lll}
		Training structures & \multicolumn{3}{c}{$ 500 $} \\
		\hline
		Type of architecture & \gls{rnn} & \gls{lstm} & \gls{gru}\\
		\hline
		Validation accuracy & $0.914$ & $0.932$ & $0.964$\\
		\hline
		Testing accuracy & $0.90$ & $0.832$ & $0.765$
	\end{tabular}
\end{table}



\begin{table}[htp]
	\centering
	\caption{Results of the fine tunning for $ 1,000 $ structures}
	\label{tab:results_fine_tuning_1000_rnn}
	\begin{tabular}{p{2.5cm}|lll}
		Training structures & \multicolumn{3}{c}{$ 1,000 $} \\
		\hline
		Type of architecture & \gls{rnn} & \gls{lstm} & \gls{gru}\\
		\hline
		Validation accuracy & $0.891$ & $0.932$ & $0.970$\\
		\hline
		Testing accuracy & $0.78$ & $0.724$ & $0.75$
	\end{tabular}
\end{table}


% Show the detailed results of the fine tuning of some networks (Paul and Thomas)

For each of the best identified network architectures of the vanilla \gls{cnn} and \gls{tcn} hyperparameter optimizations further training with \gls{lrdecay} is performed to improve the performance. The models are further refined on the same dataset on which the optimization was performed. For the models which were trained on $ 100 $ structures the models with a sliding window length of $ 30 $ are choosen for futher refinement. The optimization runs are started with the learning rate which was used in the hyperparameter optimization for the best model. From there it is decreased in steps of a decade until a learning rate of $ 10^{-5} $ is reached. The models are trained for $ 500 $ epochs with each learning rate. After the fine tuning the models are tested on the testing dataset to evaluate their final performance.

Figure \ref{fig:accuracy_adaptiveLR_CNN} depicts the training history for the fine tuning on $ 100 $, $ 500 $ and $ 1000 $ structures. The networks show little to now overfitting on the training dataset. There is steady improvement of the accuracy towards convergence except for the training on $ 500 $ structures which shows a steep drop at around $ 500 $ epochs.

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_adaptiveLR_CNN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_500_structures_adaptiveLR_CNN.pdf}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_1000_structures_adaptiveLR_CNN.pdf}}
	\caption{Training and validation accuracy history for the best models of the vanilla \gls{cnn} hyperparameter optimization on (a): $ 100 $, (b): $ 500 $ and (c): $ 1000 $ structures after fine tuning the models with \gls{lrdecay} on the same datasets}
	\label{fig:accuracy_adaptiveLR_CNN}
\end{figure}

 The training historys of the \gls{tcn} fine tuning can be seen in Figure \ref{fig:accuracy_adaptiveLR_TCN}. The best model from the hyperparameter optimization on $ 1000 $ structures was not only fine tuned on $ 1000 $ structures but also on the full dataset of $ 9900 $ structures to see if any improvements can be realised (see Figure \ref{fig:accuracy_adaptiveLR_TCN} (d)). During the first phase of training with the highest learning rate there are strong oscillations for all the models. The decrease of the learning rate helps to stabilize the training and for all the models gives a significant rise to the accuracy. In the later stages of the training all the models show a stable convergence. Only the model trained on $ 100 $ structures shows a bit of overfitting on the training dataset while the other models show no such signs. 

\begin{figure}[htp]
	\centering
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_100_structures_adaptiveLR_TCN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_500_structures_adaptiveLR_TCN.pdf}}
	\\
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_1000_structures_adaptiveLR_TCN.pdf}}
	\quad
	\subfloat[][]{\includegraphics[width=0.23\textwidth]{python/accuracy_9900_structures_adaptiveLR_TCN.pdf}}
	\caption{Training and validation accuracy history for the best models of the \gls{tcn} hyperparameter optimization on (a): $ 100 $, (b): $ 500 $, (c): $ 1000 $ and (c): $ 9900 $ structures after fine tuning the models with \gls{lrdecay} on the same datasets}
	\label{fig:accuracy_adaptiveLR_TCN}
\end{figure}

% Compare the rnn and cnn results (Paul and Thomas)

As a final validation all the fine tuned models are tested with the testing dataset of $ 100 $ structures. Table \ref{tab:accuracy_testing_rnn_cnn} shows an overview of the achieved accuracy for all \gls{rnn} (Vanilla \gls{rnn}, \gls{lstm} and \gls{gru}) and \gls{cnn} models (Vanilla \glspl{cnn} and \glspl{tcn}).

For the vanilla \gls{cnn} and \gls{tcn} models there is a steady increase in the achieved accuracy on the testing set if the amount of structures for training is increased. The increase in testing accuracy corresponds largely to the increase in validation accuracy. The testing accuracy is in the same order of magnitude as the validation accuracy indicating that the networks generalize well to new sets of data. In general \glspl{tcn} work better than \glspl{cnn} especially on the larger datasets of $ 500 $ and $ 1000 $ structures where the \gls{tcn} achieves close to $ 100 \% $ accuracy. When trained on the complete dataset the \gls{tcn} achieved $ 100 \% $ accuracy on the testing dataset.

\begin{table}[htp]
	\centering
	\caption{Accuracy of all fine tuned \gls{rnn} (Vanilla \gls{rnn}, \gls{lstm} and \gls{gru}) and \gls{cnn} models (Vanilla \glspl{cnn} and \glspl{tcn}) on the validation and testing datasets}
	\label{tab:accuracy_testing_rnn_cnn}
	\setlength{\tabcolsep}{3pt} % Default value: 6pt
	\begin{tabular}{p{2.5cm}|lll|llll}
		Model type & \multicolumn{3}{c|}{Vanilla \gls{cnn}} & \multicolumn{4}{c}{\gls{tcn}} \\
		\hline
		Number of training structures & $ 100 $ & $ 500 $ & $ 1000 $ & $ 100 $ & $ 500 $ & $ 1000 $ & $ 10000 $ \\
		Validation accuracy & $ 0.783 $ & $ 0.835 $ & $ 0.824 $ & $ 0.814 $ & $ 0.984 $ & $ 0.993 $ & $ 0.991 $ \\
		Testing accuracy & $ 0.82 $ & $ 0.84 $ & $ 0.84 $ & $ 0.82 $ & $ 0.98 $ & $ 0.99 $ & $ 1.00 $
	\end{tabular}
\end{table}


\section{Conclusions}
\label{sec:conclusions}

% Give an overview of the general results of the study

% List the results of the best models and justify the choice for a best model

% Compare the final results with the aims set at the beginning

% Give an outlook for future research on the topic


\newpage
\printbibliography

\end{document}


