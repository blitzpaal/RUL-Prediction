{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"authorship_tag":"ABX9TyPANEJEpwEJ6F5QlriUmdOl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"lFp5sqfKxEXq","executionInfo":{"status":"error","timestamp":1623665078200,"user_tz":-120,"elapsed":329065,"user":{"displayName":"Bastien Fabre","photoUrl":"","userId":"02504806516154625863"}},"outputId":"eb51032b-3f44-4f19-ec84-0e2eeb0a3030"},"source":["# Import\n","!pip3 install pickle5\n","!pip install keras-tuner\n","import pickle5 as pc\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","\n","# Import\n","import numpy as np\n","import pandas as pd\n","import json\n","#from mpi4py import MPI\n","import random\n","\n","# import keras\n","import tensorflow as tf\n","from tensorflow.keras import backend as K\n","from tensorflow.keras.models import *\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.optimizers import *\n","from tensorflow.keras.initializers import *\n","from kerastuner.tuners import *\n","from kerastuner import HyperModel\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","from datetime import datetime\n","\n","# set random seed\n","np.random.seed(1)\n","tf.random.set_seed(1)\n","\n","\n","#prepare forecasting data\n","def gen_X_sequence(id_df, seq_length, seq_cols, timesteps_pred, type_data = None):\n","    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n","    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n","    we can use shorter ones \"\"\"\n","\n","    ind_start = 0\n","    \n","    data_array = id_df[seq_cols].values\n","    num_elements = data_array.shape[0]\n","    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n","        yield data_array[start:stop, :]\n"," \n","\n","def gen_Y_sequence(id_df, seq_length, seq_cols, timesteps_pred, type_data = None):\n","    \"\"\" Only sequences that meet the window-length are considered, no padding is used. This means for testing\n","    we need to drop those which are below the window-length. An alternative would be to pad sequences so that\n","    we can use shorter ones \"\"\"\n","\n","    ind_start = 0\n","    \n","    data_array = id_df[seq_cols].values\n","    num_elements = data_array.shape[0]\n","    for start, stop in zip(range(0+ind_start, num_elements-seq_length+1-timesteps_pred), range(seq_length+ind_start, num_elements+1-timesteps_pred)):\n","        yield data_array[stop-1, :]\n","\n","   \n","def get_dataset(sequence_length, batch_size):\n","    # set folder path\n","    fd='/content/drive/MyDrive/PIR_perso'\n","    fk='/data'\n","\n","    # import data\n","    with open(fd + fk + '/data_train_v1', \"rb\") as fh:\n","      data_train_df = pc.load(fh).reset_index().iloc[:,1:]\n","    with open(fd + fk +'/data_test_v1', \"rb\") as fh:\n","      data_test_df = pc.load(fh).reset_index().iloc[:,1:]\n","\n","\n","    # data_train_df = pd.read_pickle(fd_km + '/data_train_v1').reset_index().iloc[:,1:] #full set\n","    # data_test_df = pd.read_pickle(fd_km + '/data_test_v1').reset_index().iloc[:,1:]\n","\n","    # create bins\n","    l = 0.5\n","    nb_bins = 20 # including one extra bin for RUL>upper_bin_bound\n","    lower_bin_bound = 0\n","    upper_bin_bound = 80000\n","\n","    bins = np.linspace(lower_bin_bound**l, upper_bin_bound**l, nb_bins)**(1/l)\n","    bins = np.append(bins, data_train_df.RUL.max())\n","\n","    labels=[i for i in range(bins.shape[0]-1)]\n","\n","    # categorise data\n","    data_train_df['RUL_bin'] = pd.cut(data_train_df['RUL'], bins=bins, labels=labels)\n","    data_test_df['RUL_bin'] = pd.cut(data_test_df['RUL'], bins=bins, labels=labels)\n","\n","    # build data sequences\n","    #data_train_group = [data_train_df for _, data_train_df in data_train_df.groupby('ID')]\n","    #random.shuffle(data_train_group)\n","    #data_train_df_random = pd.concat(data_train_group)\n","\n","    data_train = data_train_df[data_train_df.ID <= 500]\n","    data_val = data_train_df[data_train_df.ID > 9800]\n","    data_test = data_test_df\n","\n","    #prepare data\n","    seq_cols = ['gauge'+str(i) for i in range(1,4)]\n","    seq_cols1 = ['RUL_bin']\n","    timesteps_pred = 1\n","\n","    #training set\n","    seq_gen = (list(gen_X_sequence(data_train[data_train['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n","                    for id in data_train['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbX = np.concatenate(list(seq_gen))\n","\n","    seq_gen = (list(gen_Y_sequence(data_train[data_train['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n","                    for id in data_train['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbY = np.concatenate(list(seq_gen)).reshape(-1,)\n","\n","    #val set\n","    seq_gen = (list(gen_X_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n","                    for id in data_val['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbX_val = np.concatenate(list(seq_gen))\n","\n","    seq_gen = (list(gen_Y_sequence(data_val[data_val['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n","                    for id in data_val['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbY_val = np.concatenate(list(seq_gen)).reshape(-1,)\n","\n","    #test set\n","    seq_gen = (list(gen_X_sequence(data_test[data_test['ID']==id], sequence_length, seq_cols, timesteps_pred, type_data= 'train')) \n","                    for id in data_test['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbX_test = np.concatenate(list(seq_gen))\n","\n","    seq_gen = (list(gen_Y_sequence(data_test[data_test['ID']==id], sequence_length, seq_cols1, timesteps_pred, type_data= 'train')) \n","                    for id in data_test['ID'].unique())\n","    # generate sequences and convert to numpy array\n","    dbY_test = np.concatenate(list(seq_gen)).reshape(-1,)\n","\n","    return (\n","        tf.data.Dataset.from_tensor_slices((dbX, dbY)).batch(batch_size),\n","        tf.data.Dataset.from_tensor_slices((dbX_val, dbY_val)).batch(batch_size),\n","        tf.data.Dataset.from_tensor_slices((dbX_test, dbY_test)).batch(batch_size),\n","    )\n","\n","\n","class MyHyperModel(HyperModel):\n","\n","    def __init__(self, input_shape, output_shape):\n","        self.input_shape = input_shape\n","        self.output_shape = output_shape\n","\n","    def build(self, hp):\n","        # build model\n","        input_layer = Input(shape=self.input_shape)\n","\n","        x = LayerNormalization(axis=1)(input_layer)\n","        x = SimpleRNN(32, dropout=0, recurrent_dropout=0, return_sequences=True)(x)\n","        x = SimpleRNN(32, dropout=0, recurrent_dropout=0, return_sequences=True)(x)\n","        x = SimpleRNN(32, dropout=0, recurrent_dropout=0, return_sequences=False)(x)\n","        x = Dense(self.output_shape, activation='softmax')(x)\n","        output_layer = x\n","\n","        model = Model(input_layer, output_layer)\n","\n","        # compile model\n","        model.compile(\n","            optimizer=Adam(),\n","            loss='sparse_categorical_crossentropy',\n","            metrics=['SparseCategoricalAccuracy'])\n","\n","        return model\n","\n","\n","# Load training, validation and test data\n","batch_size = 4096\n","sequence_length = 30\n","train_dataset, val_dataset, test_dataset = get_dataset(\n","    sequence_length=sequence_length, batch_size=batch_size)\n","\n","\n","# Create a MirroredStrategy.\n","strategy = tf.distribute.MirroredStrategy()\n","print(\"Number of devices: {}\".format(strategy.num_replicas_in_sync))\n","\n","# Open a strategy scope.\n","with strategy.scope():\n","    # Everything that creates variables should be under the strategy scope.\n","    # In general this is only model construction & `compile()`.\n","    model = build_model(input_shape = (sequence_length, 3), output_shape = 20)\n","\n","# model_directory = 'CNN_Model3_6' + '/' + '100_structures_adaptiveLR_1'\n","# model_path = model_directory + '/' + 'CNN_Model3_6_adaptiveLR_1'\n","\n","# # get model as json string and save to file\n","# model_as_json = model.to_json()\n","# with open(model_path + '.json', \"w\") as json_file:\n","#     json_file.write(model_as_json)\n","\n","es = keras.callbacks.EarlyStopping(monitor='sparse_categorical_accuracy', min_delta=0, patience=100, verbose=2, mode='max')\n","mc = keras.callbacks.ModelCheckpoint('best_model_rnn_prob.h5', monitor='sparse_categorical_accuracy', mode='max', \n","                                     save_weights_only=True, save_best_only=True)\n","# tb = tf.keras.callbacks.TensorBoard(model_directory)\n","\n","for lr in [0.01]:\n","    model.compile(\n","            optimizer=Adam(lr),\n","            loss='sparse_categorical_crossentropy',\n","            metrics=['SparseCategoricalAccuracy'])\n","\n","    # Train the model on all available devices.\n","    history = model.fit(train_dataset, epochs=500, verbose=2, validation_data=val_dataset, callbacks = [mc])"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pickle5 in /usr/local/lib/python3.7/dist-packages (0.0.11)\n","Requirement already satisfied: keras-tuner in /usr/local/lib/python3.7/dist-packages (1.0.2)\n","Requirement already satisfied: terminaltables in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (3.1.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.16.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.22.2.post1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.19.5)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.4.4)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (0.8.9)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (20.9)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2020.12.5)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->keras-tuner) (1.0.1)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (2.4.7)\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n","INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n","Number of devices: 1\n","Epoch 1/500\n","14/14 - 8s - loss: 2.8360 - sparse_categorical_accuracy: 0.0785 - val_loss: 2.6725 - val_sparse_categorical_accuracy: 0.1011\n","Epoch 2/500\n","14/14 - 2s - loss: 2.6389 - sparse_categorical_accuracy: 0.1082 - val_loss: 2.6338 - val_sparse_categorical_accuracy: 0.1011\n","Epoch 3/500\n","14/14 - 2s - loss: 2.6211 - sparse_categorical_accuracy: 0.1084 - val_loss: 2.6247 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 4/500\n","14/14 - 2s - loss: 2.6185 - sparse_categorical_accuracy: 0.1109 - val_loss: 2.6238 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 5/500\n","14/14 - 2s - loss: 2.6165 - sparse_categorical_accuracy: 0.1106 - val_loss: 2.6224 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 6/500\n","14/14 - 2s - loss: 2.6154 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6225 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 7/500\n","14/14 - 2s - loss: 2.6149 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6221 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 8/500\n","14/14 - 2s - loss: 2.6147 - sparse_categorical_accuracy: 0.1115 - val_loss: 2.6221 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 9/500\n","14/14 - 2s - loss: 2.6146 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 10/500\n","14/14 - 2s - loss: 2.6146 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 11/500\n","14/14 - 2s - loss: 2.6145 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 12/500\n","14/14 - 2s - loss: 2.6145 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 13/500\n","14/14 - 2s - loss: 2.6144 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 14/500\n","14/14 - 2s - loss: 2.6144 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 15/500\n","14/14 - 2s - loss: 2.6144 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 16/500\n","14/14 - 2s - loss: 2.6144 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6221 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 17/500\n","14/14 - 2s - loss: 2.6143 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6221 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 18/500\n","14/14 - 2s - loss: 2.6143 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6221 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 19/500\n","14/14 - 2s - loss: 2.6143 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6222 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 20/500\n","14/14 - 2s - loss: 2.6143 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6222 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 21/500\n","14/14 - 2s - loss: 2.6143 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6222 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 22/500\n","14/14 - 2s - loss: 2.6142 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6223 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 23/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6231 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 24/500\n","14/14 - 2s - loss: 2.6159 - sparse_categorical_accuracy: 0.1119 - val_loss: 2.6227 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 25/500\n","14/14 - 2s - loss: 2.6528 - sparse_categorical_accuracy: 0.1099 - val_loss: 2.6977 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 26/500\n","14/14 - 2s - loss: 2.6687 - sparse_categorical_accuracy: 0.1022 - val_loss: 2.6497 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 27/500\n","14/14 - 2s - loss: 2.7101 - sparse_categorical_accuracy: 0.0984 - val_loss: 2.7009 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 28/500\n","14/14 - 2s - loss: 2.6465 - sparse_categorical_accuracy: 0.1021 - val_loss: 2.6323 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 29/500\n","14/14 - 2s - loss: 2.6218 - sparse_categorical_accuracy: 0.1085 - val_loss: 2.6238 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 30/500\n","14/14 - 2s - loss: 2.6161 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6226 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 31/500\n","14/14 - 2s - loss: 2.6149 - sparse_categorical_accuracy: 0.1109 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 32/500\n","14/14 - 2s - loss: 2.6144 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 33/500\n","14/14 - 2s - loss: 2.6142 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 34/500\n","14/14 - 2s - loss: 2.6141 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 35/500\n","14/14 - 2s - loss: 2.6140 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 36/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 37/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 38/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 39/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 40/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 41/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 42/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1126 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 43/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 44/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 45/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 46/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 47/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 48/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6217 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 49/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1121 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 50/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 51/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 52/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 53/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 54/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 55/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 56/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 57/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 58/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1118 - val_loss: 2.6218 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 59/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1116 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 60/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1116 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 61/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1116 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 62/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1116 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 63/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 64/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 65/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 66/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 67/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 68/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 69/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 70/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 71/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 72/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 73/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 74/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 75/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 76/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 77/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 78/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 79/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 80/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 81/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 82/500\n","14/14 - 2s - loss: 2.6137 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 83/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 84/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1112 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 85/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 86/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 87/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 88/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 89/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 90/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 91/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 92/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 93/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 94/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 95/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 96/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 97/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 98/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 99/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 100/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 101/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 102/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 103/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 104/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 105/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 106/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 107/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 108/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 109/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6219 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 110/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 111/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 112/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 113/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 114/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 115/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1110 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 116/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 117/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 118/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 119/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 120/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 121/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 122/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 123/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 124/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 125/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 126/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 127/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 128/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 129/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 130/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 131/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 132/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 133/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 134/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 135/500\n","14/14 - 2s - loss: 2.6138 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 136/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1119\n","Epoch 137/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1111 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 138/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 139/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 140/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 141/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 142/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 143/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 144/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 145/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 146/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 147/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 148/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 149/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 150/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 151/500\n","14/14 - 2s - loss: 2.6139 - sparse_categorical_accuracy: 0.1107 - val_loss: 2.6220 - val_sparse_categorical_accuracy: 0.1095\n","Epoch 152/500\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-d6d689012051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;31m# Train the model on all available devices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3024\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1961\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}